{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c01239dc",
   "metadata": {},
   "source": [
    "### Stage 3 NDG <br>\n",
    "\n",
    "Collect the users from NDG which didn't have the word 'depress' \n",
    "1. the <br>\n",
    "2. i <br>\n",
    "3. to <br>\n",
    "4. a <br> \n",
    "5. and <br>\n",
    "6. is <br>\n",
    "7. in <br>\n",
    "8. it <br>\n",
    "9. you <br>\n",
    "10. of <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e39cdee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7519bb88",
   "metadata": {},
   "outputs": [],
   "source": [
    "twitterid = ['supershuaifeng','']\n",
    "twitterId = twitterid[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e931e4ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 swfs4x4\n",
      "5 JTApeMoon\n",
      "8 kf6spf\n",
      "10 Bad__Scooter\n",
      "11 ranoush333\n",
      "12 LaikaAndYuri\n",
      "14 JudgyJudes\n",
      "15 JazzyToU2\n",
      "17 golow_jay\n",
      "18 EliasLaradi\n",
      "19 lyleanthony_usa\n",
      "21 AcademicsCole\n",
      "22 villanevill\n",
      "25 yourgagarazzi\n",
      "26 BathBombWhale\n",
      "29 sabakhanjatoi\n",
      "30 ElDainosor\n",
      "31 manisra47\n",
      "32 BehemothKnight\n",
      "33 Edourdoo\n",
      "34 Cardisinal\n",
      "35 IamHolid\n",
      "36 Raybuggybug\n",
      "38 SakLifeLeeski\n",
      "40 JasolaLawal\n",
      "41 beetrivia\n",
      "42 seobskz\n",
      "43 Hyuktankhamun\n",
      "44 ImVeryUncool_\n",
      "45 pennzoilmy\n",
      "47 ClaytonnManning\n",
      "49 kokoro_0708\n",
      "50 EmbersAbsolve\n",
      "52 daninotacroc\n",
      "53 swwqadie\n",
      "55 pman4us\n",
      "57 vicerabot\n",
      "61 CashBSpazzin\n",
      "62 DemonSynth\n",
      "63 XFaure\n",
      "65 stibbzy8\n",
      "66 duchessofpigs\n",
      "68 WorriedOf\n",
      "69 bionicman112\n",
      "70 jeiting\n",
      "72 Trevor_Pitt\n",
      "73 LightBluish95\n",
      "74 jawnlouis\n",
      "75 yoooo_jelly\n",
      "76 jbrown3079\n",
      "77 JimLNeibaur\n",
      "79 liyah_kb\n",
      "80 m4x_44\n",
      "81 BEAT_SKELTON\n",
      "82 miss_myaaaa\n",
      "83 CassieSGolden\n",
      "84 Ohioman2010\n",
      "85 _casuali\n",
      "86 BadMaditude\n",
      "87 channi_crawford\n",
      "88 whoissegun\n",
      "89 AgentOrangeBTC\n",
      "90 dondondongie\n",
      "91 NotThato\n",
      "92 JregPeg\n",
      "98 perrywtoon\n",
      "99 SummertimeGee\n",
      "Done for NDG with word depress.csv\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Stage 2 NonDepressionGroup_V2.csv\")\n",
    "twitterid = data.TwitterID\n",
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"Stage 3/Non Depression Group/V2/NDG_\" + str(index) + \"_\" + id +\".csv\"\n",
    "    \n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "            print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"ID\",\"Username\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"NDG with word depress.csv\"\n",
    "tweets_df.to_csv(file_name,index=False)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bb352daf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 CoopDuBois\n",
      "1 DragonImagining\n",
      "2 AnaaSilvia\n",
      "3 BABYCNBOARD\n",
      "6 9YinG69YanG6\n",
      "7 Karzeuha\n",
      "9 abelationsxo\n",
      "13 brazedbrand\n",
      "16 wesleymiller360\n",
      "20 ChimeraSea_\n",
      "23 monknom44\n",
      "24 iamcb5\n",
      "27 Jaylen_Areyon\n",
      "28 HaruchiBee\n",
      "37 CyberRori\n",
      "39 Be_Rightvc\n",
      "46 newcastleis\n",
      "48 TropicalSpanish\n",
      "51 hourlywattson\n",
      "54 lemonyletter\n",
      "56 SwagaGunner\n",
      "58 adebayo_merit\n",
      "59 ammisays\n",
      "60 RamanDe31443197\n",
      "64 exomoroll\n",
      "67 shazx123\n",
      "71 sdfifty2\n",
      "78 ChaChaPersian\n",
      "93 gErwitnMonney\n",
      "94 djmcbreen\n",
      "95 mmazik10\n",
      "96 CantH0ldMe_\n",
      "97 Cfccd14\n",
      "Done for #NDG V2 with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"Stage 2 NonDepressionGroup_V2.csv\")\n",
    "twitterid = data.TwitterID\n",
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"Stage 3/Non Depression Group/V2/NDG_\" + str(index) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#NDG V2 with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5409e24e",
   "metadata": {},
   "source": [
    "A with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "15ae4cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10       parkdaler109\n",
      "11    jspence32208074\n",
      "12          thejareax\n",
      "13     pettyblackgirI\n",
      "14         woahhhhgan\n",
      "15       myaaadabraat\n",
      "16       PKMNTownsBot\n",
      "17      love_taylorr_\n",
      "18       UnsortedMent\n",
      "19     BfReflectionJM\n",
      "20           Mvnchman\n",
      "21    JunctionJunebug\n",
      "22       aquariancity\n",
      "23        Stephyyy_26\n",
      "24          a46786384\n",
      "25           o_a_khan\n",
      "26      BakedBeanAlex\n",
      "27             zoomin\n",
      "28        SexHealthEd\n",
      "29        Bunniestvau\n",
      "30          bonbonb0t\n",
      "31     _niggasAinSHXT\n",
      "32           SLIMYNIP\n",
      "33      PrincessCoyaa\n",
      "34             jazeua\n",
      "35         the_omegin\n",
      "36          frncn_rmc\n",
      "37         TELFARDOLL\n",
      "38          Ulann0327\n",
      "Name: TwitterID, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_a.csv\")\n",
    "twitterid = data.TwitterID[10:39]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1251016",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 thejareax\n",
      "4 woahhhhgan\n",
      "6 PKMNTownsBot\n",
      "9 BfReflectionJM\n",
      "14 a46786384\n",
      "16 BakedBeanAlex\n",
      "17 zoomin\n",
      "19 Bunniestvau\n",
      "20 bonbonb0t\n",
      "24 jazeua\n",
      "26 frncn_rmc\n",
      "27 TELFARDOLL\n",
      "28 Ulann0327\n",
      "Done for #A with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"a/NDG_a_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#A with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc4404d",
   "metadata": {},
   "source": [
    "AND with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f65663b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10        JazzyToU2\n",
      "11          theGSBI\n",
      "12         fenamjim\n",
      "13     MalarkeyCali\n",
      "14    jon_flop_boat\n",
      "          ...      \n",
      "73      Eggstickles\n",
      "74    tessaspeaknow\n",
      "75     mathslab2020\n",
      "76        esterx01W\n",
      "77     Infamouskelv\n",
      "Name: TwitterID, Length: 68, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_and.csv\")\n",
    "twitterid = data.TwitterID[10:78]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b475fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 theGSBI\n",
      "2 fenamjim\n",
      "3 MalarkeyCali\n",
      "8 FRAKT_Updates\n",
      "12 dtnn4lif33\n",
      "16 ovoadryana1\n",
      "19 mohdshehab10_\n",
      "20 DeadBirain\n",
      "21 KristopherHell4\n",
      "22 PrahladSaw28\n",
      "24 abikokatsunori\n",
      "28 jonginsinner\n",
      "31 megalodcn\n",
      "32 Kulsumwindxmart\n",
      "36 mwrich87\n",
      "40 Xynezone\n",
      "42 ashrooh890\n",
      "45 FFFfinanceblog\n",
      "47 labiquack\n",
      "50 newzz4u\n",
      "55 oobysnaccin\n",
      "56 likejjongs\n",
      "57 Aaron_4421\n",
      "58 Clitortoise\n",
      "59 LeMichael__\n",
      "61 wulfqnq\n",
      "65 mathslab2020\n",
      "66 esterx01W\n",
      "67 Infamouskelv\n",
      "Done for #AND with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"and/NDG_and_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#AND with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafe8535",
   "metadata": {},
   "source": [
    "I with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6b27677f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10       AadityaN_28\n",
      "11        retromissa\n",
      "12       myoongitrbl\n",
      "13    PanchamKhaitan\n",
      "14          Xindrooo\n",
      "           ...      \n",
      "77     hayleeddababy\n",
      "78         kayykiraa\n",
      "79          xosworld\n",
      "80         __KeeSlim\n",
      "81          xGodofMe\n",
      "Name: TwitterID, Length: 72, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_i.csv\")\n",
    "twitterid = data.TwitterID[10:82]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "dfa361c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 retromissa\n",
      "12 myoongitrbl\n",
      "13 PanchamKhaitan\n",
      "14 Xindrooo\n",
      "28 seowonliker\n",
      "31 13OZZZ\n",
      "33 brucerogers666\n",
      "45 spvnishshawwty\n",
      "48 TylerOfAmerica\n",
      "51 samjones_ugc\n",
      "52 Ing_Agri\n",
      "53 TapestryofScarz\n",
      "60 Nayaa1x\n",
      "63 abbageI\n",
      "66 vfxwluv\n",
      "71 dahyunbear\n",
      "72 shmily_pp\n",
      "75 cirroxyz\n",
      "79 xosworld\n",
      "Done for #I with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"i/NDG_i_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index+10)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#I with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aaf690f",
   "metadata": {},
   "source": [
    "IN with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "215f351f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10         Cardisinal\n",
      "11           seyi_oso\n",
      "12      ShopSJSHockey\n",
      "13             PP9985\n",
      "14        MaduOnuorah\n",
      "           ...       \n",
      "82     AylesburyMeths\n",
      "83          Sani_Sy__\n",
      "84    Shehrya04929514\n",
      "85        SoBoldOfYou\n",
      "86     JafferyAcademy\n",
      "Name: TwitterID, Length: 77, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_in.csv\")\n",
    "twitterid = data.TwitterID[10:87]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf54d75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 seyi_oso\n",
      "12 ShopSJSHockey\n",
      "15 jeon_bora_tae\n",
      "17 RAMmagazineOz\n",
      "19 gabriellamadd\n",
      "23 sandybaca156\n",
      "25 ransonfamilypix\n",
      "27 abdullahiyaro15\n",
      "28 boboyout1\n",
      "29 billhero\n",
      "34 farhan_hossain9\n",
      "36 1t_cartier\n",
      "39 thefamedpj\n",
      "42 miisteraa\n",
      "44 GailDianne1\n",
      "45 AliceMukamutesi\n",
      "49 PSArekt\n",
      "56 blsp_india\n",
      "57 jadsfc\n",
      "60 Favouredchudi\n",
      "62 yazzerOsman\n",
      "65 Polly_Obscurum\n",
      "66 willows1234\n",
      "68 zendayasblazer\n",
      "73 oscarkimbauman\n",
      "74 sanisnani\n",
      "76 happiehao\n",
      "79 RavenKnight333\n",
      "81 SatomiBOT\n",
      "83 Sani_Sy__\n",
      "84 Shehrya04929514\n",
      "86 JafferyAcademy\n",
      "Done for #IN with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"in/NDG_in_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index+10)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#IN with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f70c6f8",
   "metadata": {},
   "source": [
    "IS with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "bfd6bb18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10    ClaytonnManning\n",
      "11       amarjeet_del\n",
      "12       uptourismgov\n",
      "13     DeeOneAyekooto\n",
      "14      SassyandGeeky\n",
      "           ...       \n",
      "79      rohit_pradhan\n",
      "80    aestheticmessme\n",
      "81         thefamedpj\n",
      "82           xoxoo_de\n",
      "83    Attentive_Being\n",
      "Name: TwitterID, Length: 74, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_is.csv\")\n",
    "twitterid = data.TwitterID[10:84]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d1d823bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 uptourismgov\n",
      "15 BarrazaNiven\n",
      "17 rizzacoloma13\n",
      "21 LuthoMatiwane3\n",
      "23 ezeky_shaw\n",
      "25 LiselotFontene1\n",
      "26 if_onlly\n",
      "28 dias_chriss\n",
      "32 RobertoyawBrako\n",
      "33 MountMoriahSeed\n",
      "35 LouthMoriah\n",
      "37 cansfordaysmate\n",
      "40 MummaLovesBTS\n",
      "42 Jamiyaaa01\n",
      "45 itsbumblebear\n",
      "46 vigilantLEO\n",
      "52 m_shutterbug\n",
      "53 BeautifulLEXX_\n",
      "54 BillShanks\n",
      "55 lilioflu\n",
      "56 EverydayMile\n",
      "59 Clergi_M\n",
      "61 Shaizeen12\n",
      "62 NovePlusNove\n",
      "63 Sorandom_nece\n",
      "64 SydneyNicolex_\n",
      "70 phtechy\n",
      "72 AddelHeersink\n",
      "76 Richard51499225\n",
      "79 rohit_pradhan\n",
      "81 thefamedpj\n",
      "Done for #IS with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"is/NDG_is_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index+10)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#IS with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64595ad",
   "metadata": {},
   "source": [
    "IT with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7dd6f844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10      EmbersAbsolve\n",
      "11       daninotacroc\n",
      "12      adebayo_merit\n",
      "13     NusaibahOnline\n",
      "14    matrix100jaguar\n",
      "15            bnuuyes\n",
      "16        GodsOwns004\n",
      "17        gamechunktv\n",
      "18        finalneedle\n",
      "19     AshengrottoBot\n",
      "20        OKAYYYWOWWW\n",
      "21        sadbitch107\n",
      "22    AyndaMthimkhulu\n",
      "23     callmedonvilla\n",
      "24             T02dd1\n",
      "25    dogboyification\n",
      "26           Y01M1Y4Z\n",
      "27        BasedSupoko\n",
      "28       MyaaHendrixx\n",
      "29      ririe20674754\n",
      "30         mangitolai\n",
      "31     Therealkee____\n",
      "32          DevlinSyo\n",
      "33           LiSzaSzn\n",
      "34            teyloh1\n",
      "35        WOLVESCL0AK\n",
      "36       AnthonyJeff6\n",
      "37             lvxmin\n",
      "38          KirbyTVLK\n",
      "39           KayRoss4\n",
      "40           StayCeeS\n",
      "41           asterwsh\n",
      "42       angelodenico\n",
      "43       leoslyricbot\n",
      "44         blindllxll\n",
      "45    urfavoritejamie\n",
      "Name: TwitterID, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_it.csv\")\n",
    "twitterid = data.TwitterID[10:46]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cf47a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12 adebayo_merit\n",
      "13 NusaibahOnline\n",
      "17 gamechunktv\n",
      "19 AshengrottoBot\n",
      "23 callmedonvilla\n",
      "24 T02dd1\n",
      "26 Y01M1Y4Z\n",
      "30 mangitolai\n",
      "31 Therealkee____\n",
      "33 LiSzaSzn\n",
      "35 WOLVESCL0AK\n",
      "41 asterwsh\n",
      "42 angelodenico\n",
      "43 leoslyricbot\n",
      "45 urfavoritejamie\n",
      "Done for #IT with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"it/NDG_it_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index+10)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#IT with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d69a27",
   "metadata": {},
   "source": [
    "OF with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "194315c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10             XFaure\n",
      "11    nickolson_terry\n",
      "12      7NEWSCentWest\n",
      "13        FinAmourBot\n",
      "14    CrockersMarquee\n",
      "15      ItsKieranDrew\n",
      "16    adrenaline_mask\n",
      "17       yvestumorbot\n",
      "18    CONSTAN70659172\n",
      "19    Digitalmaarifa_\n",
      "20      PatrioticOn22\n",
      "21    Knight_of_Disks\n",
      "22      davidparry100\n",
      "23        Eddie_Gore1\n",
      "24        gialloesque\n",
      "25        wessaquotes\n",
      "26         kuebanlynk\n",
      "27          hourlyhnk\n",
      "28          SFredabel\n",
      "29            rexuIti\n",
      "30     SolomonGrayTTV\n",
      "31         blvckledge\n",
      "32     yubyubcommandr\n",
      "33    ashvinmalviya10\n",
      "34       malachitebot\n",
      "35    NOLANGRAYSONBOT\n",
      "36        JUST_KWANDA\n",
      "37      WhoPotterVian\n",
      "38           grealyeo\n",
      "39           melafela\n",
      "40        loverboybot\n",
      "41            JSchuks\n",
      "42          SaiyaNull\n",
      "43         tslyricb0t\n",
      "44     tpwk4eva112302\n",
      "45    Shugharbeautye1\n",
      "46       doIlbehavior\n",
      "Name: TwitterID, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_of.csv\")\n",
    "twitterid = data.TwitterID[10:47]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "df185810",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 FinAmourBot\n",
      "14 CrockersMarquee\n",
      "18 CONSTAN70659172\n",
      "19 Digitalmaarifa_\n",
      "23 Eddie_Gore1\n",
      "25 wessaquotes\n",
      "26 kuebanlynk\n",
      "27 hourlyhnk\n",
      "28 SFredabel\n",
      "29 rexuIti\n",
      "30 SolomonGrayTTV\n",
      "35 NOLANGRAYSONBOT\n",
      "38 grealyeo\n",
      "40 loverboybot\n",
      "44 tpwk4eva112302\n",
      "46 doIlbehavior\n",
      "Done for #OF with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"of/NDG_of_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index+10)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#OF with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510b1122",
   "metadata": {},
   "source": [
    "THE with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "658049f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10         LiveRocks1\n",
      "11           sdfifty2\n",
      "12        p3psidadbod\n",
      "13            TAN1AAA\n",
      "14               WCVB\n",
      "15            Joslynm\n",
      "16      KoyiguriKumar\n",
      "17             owil71\n",
      "18    Boredandtiredok\n",
      "19    Rajeshw91787904\n",
      "20           scrump_1\n",
      "21      glasneronfilm\n",
      "22             DVDJXX\n",
      "23         Grigthrall\n",
      "24       kelli_linn15\n",
      "25     WERANowPlaying\n",
      "26           hofmeyr_\n",
      "27     spiritandseoul\n",
      "28    exfundyfeminist\n",
      "29          joedawson\n",
      "30      alannalaprima\n",
      "31    _pizzahutbuffet\n",
      "32           Trabeees\n",
      "33        paoacflores\n",
      "34     rhiannonkatrin\n",
      "35       WillHollon14\n",
      "36       TheSopranbro\n",
      "37            allmurf\n",
      "38        IDKWhatPut_\n",
      "39       FreedomFinLB\n",
      "40       bowtiedkobra\n",
      "41          abragailb\n",
      "42        Paddy_Stash\n",
      "43         FloydGonda\n",
      "44         GMhysterix\n",
      "45    polksaladcinema\n",
      "46     depechebigmood\n",
      "47     BL00DANDEMPIRE\n",
      "48        IAmSkweetis\n",
      "49        itsnathrdzz\n",
      "50    AmandaSteinbru1\n",
      "51           _TheBenn\n",
      "52           TKDisco8\n",
      "53        TheCineBoiz\n",
      "54           Boss3366\n",
      "55     fairlyoddoddly\n",
      "56      nflstreamtv89\n",
      "57        bloddy_snow\n",
      "58           JAMvsJAM\n",
      "59      EvilAriLennox\n",
      "60         7errenceli\n",
      "61           1Mexico5\n",
      "62        canenocandy\n",
      "63      ReavesTheGoat\n",
      "64    watermeIonsvgar\n",
      "65     danyialashunn_\n",
      "66         godisaduda\n",
      "67          mirohpink\n",
      "68      alltherowbots\n",
      "69    sakurablossomx2\n",
      "Name: TwitterID, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_the.csv\")\n",
    "twitterid = data.TwitterID[10:70]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "74502e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 LiveRocks1\n",
      "11 sdfifty2\n",
      "12 p3psidadbod\n",
      "16 KoyiguriKumar\n",
      "17 owil71\n",
      "19 Rajeshw91787904\n",
      "26 hofmeyr_\n",
      "36 TheSopranbro\n",
      "37 allmurf\n",
      "39 FreedomFinLB\n",
      "43 FloydGonda\n",
      "48 IAmSkweetis\n",
      "49 itsnathrdzz\n",
      "50 AmandaSteinbru1\n",
      "56 nflstreamtv89\n",
      "59 EvilAriLennox\n",
      "60 7errenceli\n",
      "61 1Mexico5\n",
      "68 alltherowbots\n",
      "Done for #THE with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"the/NDG_the_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index+10)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#THE with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad25fcd",
   "metadata": {},
   "source": [
    "To with no depress word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b7bd9e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10         whoissegun\n",
      "11          devon_nrg\n",
      "12      Harley_Karley\n",
      "13          themeteor\n",
      "14      JithinJohn_98\n",
      "15       DASH_Skating\n",
      "16     AimsDigitalNet\n",
      "17           yifasurd\n",
      "18            huja331\n",
      "19        _Otienolizz\n",
      "20        itohanspace\n",
      "21        maliacrider\n",
      "22        gleaningway\n",
      "23          kitsch444\n",
      "24         baynegibby\n",
      "25         lserver362\n",
      "26           CryptUwU\n",
      "27    ChrishantiniAn2\n",
      "28       sapphic_ship\n",
      "29           betheevo\n",
      "30           Billando\n",
      "31     ThatProgrammer\n",
      "32        maeisafraid\n",
      "33         Tonicky123\n",
      "34      IHindirapuram\n",
      "35              dpyo_\n",
      "36      Rohit_Bond007\n",
      "37       summerelaine\n",
      "Name: TwitterID, dtype: object\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"RandonNonDepression_to.csv\")\n",
    "twitterid = data.TwitterID[10:38]\n",
    "print(twitterid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "97434ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11 devon_nrg\n",
      "14 JithinJohn_98\n",
      "15 DASH_Skating\n",
      "18 huja331\n",
      "19 _Otienolizz\n",
      "20 itohanspace\n",
      "21 maliacrider\n",
      "22 gleaningway\n",
      "27 ChrishantiniAn2\n",
      "34 IHindirapuram\n",
      "35 dpyo_\n",
      "36 Rohit_Bond007\n",
      "Done for #TO with no word depress.csv\n"
     ]
    }
   ],
   "source": [
    "attributes_container = []\n",
    "\n",
    "for index,id in enumerate(twitterid):\n",
    "    filename = \"to/NDG_to_\" + str(index+10) + \"_\" + id +\".csv\"\n",
    "    UserDepress = False\n",
    "    NDGUser = pd.read_csv(filename)\n",
    "#     print(filename)\n",
    "#     print(NDGUser['TweetContent'])\n",
    "    \n",
    "    UserTweetContent = NDGUser['TweetContent']\n",
    "    \n",
    "    for tweet in UserTweetContent:\n",
    "        if re.search('depress.+', tweet.lower()) :\n",
    "            UserDepress = True\n",
    "#             attributes_container.append({\"ID\":index,\"Username\":id,\"TweetContent\":tweet})\n",
    "#             print (str(index) + \" \" +id  +\":\" +tweet)\n",
    "#             print(str(index)+ \" \" +id )\n",
    "            break\n",
    "    \n",
    "    if UserDepress == False:\n",
    "        print(str(index+10)+ \" \" +id )\n",
    "        attributes_container.append({\"TwitterID\":id,\"TweetContent\":tweet})\n",
    "    \n",
    "tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\",\"TweetContent\"])\n",
    "\n",
    "file_name = \"#TO with no word depress.csv\"\n",
    "tweets_df.to_csv(file_name)\n",
    "print(\"Done for \" + file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ec8ccc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831e7e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3959b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679a050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
