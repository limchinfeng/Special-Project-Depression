{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f6f537",
   "metadata": {},
   "source": [
    "### Stage 1 NDG <br>\n",
    "\n",
    "Collect 5000 user that contain the keyword <br><br>\n",
    "1. the <br>\n",
    "2. i <br>\n",
    "3. to <br>\n",
    "4. a <br> \n",
    "5. and <br>\n",
    "6. is <br>\n",
    "7. in <br>\n",
    "8. it <br>\n",
    "9. you <br>\n",
    "10. of <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "196bf6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "import enchant\n",
    "dictionary = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6192b03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Old code that will collect the tweets which is other language\n",
    "\n",
    "# #create noise data\n",
    "# keywords = ['the','i','to','a','and','is','in','it','you','of']\n",
    "\n",
    "# #Created a list to append all tweet attributes(data)\n",
    "\n",
    "# for keyword in keywords:\n",
    "    \n",
    "#     attributes_container = []\n",
    "#     key = '\"' + keyword + '\"'\n",
    "    \n",
    "#     # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "#     for i,tweet in enumerate(sntwitter.TwitterSearchScraper(key).get_items()):\n",
    "\n",
    "#         if i==5000:\n",
    "#             break\n",
    "#         attributes_container.append({\"TwitterID\":tweet.user.username, \"Username\":tweet.user.displayname, \n",
    "#                                      \"Keyword\":keyword, \"DateDepressionReported\":str(tweet.date.day)+\"-\"+str(tweet.date.month)+\"-\"+str(tweet.date.year), \n",
    "#                                      \"ActualTweet\":tweet.url,\"TweetContent\":tweet.rawContent})\n",
    "\n",
    "\n",
    "#     # Creating a dataframe from the tweets list above \n",
    "#     tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\", \"Username\", \"Keyword\", \"DateDepressionReported\",\"ActualTweet\",\"TweetContent\"])\n",
    "\n",
    "#     file_name = \"nonDepressionKeyword_\"+keyword+\".csv\"\n",
    "#     tweets_df.to_csv(file_name,index=False)\n",
    "#     print(\"Done for \" + keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4eb56a27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for 'the'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping tweet 1635125789023944704 which is not in globalObjects\n",
      "Skipping tweet 1635125181818470401 which is not in globalObjects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for 'i'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping tweet 1635130746099892226 which is not in globalObjects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done for 'to'\n",
      "Done for 'a'\n",
      "Done for 'and'\n",
      "Done for 'is'\n",
      "Done for 'in'\n",
      "Done for 'it'\n",
      "Done for 'you'\n",
      "Done for 'of'\n"
     ]
    }
   ],
   "source": [
    "#RUN THIS \n",
    "\n",
    "keywords = ['the','i','to','a','and','is','in','it','you','of']\n",
    "# Created a list to append all tweet attributes(data)\n",
    "for keyword in keywords:\n",
    "    \n",
    "    attributes_container = []\n",
    "    key = '\"' + keyword + '\"'\n",
    "    count = 0\n",
    "    # Using TwitterSearchScraper to scrape data and append tweets to list\n",
    "    for tweet in sntwitter.TwitterSearchScraper(key).get_items():\n",
    "            \n",
    "        if count==5000:\n",
    "            break\n",
    "            \n",
    "        text = tweet.rawContent\n",
    "        splitted_text = text.split()\n",
    "        if(len(text.split()) >= 5):\n",
    "            if (dictionary.check(splitted_text[0]) and dictionary.check(splitted_text[1]) and dictionary.check(splitted_text[2]) and dictionary.check(splitted_text[3]) and dictionary.check(splitted_text[4])):\n",
    "                attributes_container.append({\"TwitterID\":tweet.user.username, \"Username\":tweet.user.displayname, \n",
    "                                         \"Keyword\":keyword, \"DateDepressionReported\":str(tweet.date.day)+\"-\"+str(tweet.date.month)+\"-\"+str(tweet.date.year), \n",
    "                                         \"ActualTweet\":tweet.url,\"TweetContent\":tweet.rawContent})\n",
    "                count+=1\n",
    "\n",
    "\n",
    "    # Creating a dataframe from the tweets list above \n",
    "    tweets_df = pd.DataFrame(attributes_container, columns=[\"TwitterID\", \"Username\", \"Keyword\", \"DateDepressionReported\",\"ActualTweet\",\"TweetContent\"])\n",
    "\n",
    "    file_name = \"nonDepressionKeyword_\"+keyword+\".csv\"\n",
    "    tweets_df.to_csv(file_name,index=False)\n",
    "    print(\"Done for '\" + keyword + \"'\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
